Bias in AI 

Have you ever thought of an AI as a perfectly fair, robot judge that always makes the right choice? It’s a common mistake! In reality, AI systems are socio-technical systems, which means they are not neutral tools; they reflect the goals, values, and even the hidden opinions of the humans who build them. 

To understand why AI isn't always objective, we can look at three main areas: training data, design choices and diversity. 

1. Training Data: "Learning by Example" 

Most AI uses supervised machine learning, where it learns how to make decisions by looking at thousands of examples called training data. If this data is biased or incomplete, the AI will "learn" those same mistakes. 

Representation Bias: If a "cat-dog classifier" is trained on 100 pictures of cats but only 5 pictures of dogs, it will be much better at identifying cats and might wrongly guess that a dog is a cat. 

Historical Bias: AI often learns from data that reflects past real-world prejudices. For example, a famous Amazon hiring tool was found to be biased against women because it was trained on resumes from a decade when most tech jobs were held by men. 

The "Teenager" Problem: A study by the University of Washington found that AI models often have negative associations with teenagers. While teens describe their lives as "mundane" (video games, hanging with friends), AI models often associate them with violence or drug use because the models were trained on negative news headlines rather than actual teen experiences. 

2. Design Choices: What is the Goal? 

AI doesn't just "think" on its own; humans decide what the AI is optimizing for. Optimization is the "true goal" the programmer sets for the system. 

 Profit vs. People: An app like YouTube is advertised as a way to entertain you, but its algorithm is actually optimized to make a profit by keeping you watching as long as possible so you see more ads. 

Ranking Bias: Search engines often use ranking bias, where they put certain results at the top. Users are trained to believe the top results are the "best," even if they are just the most popular or the ones that make the company more money. 

3. Diversity: Who is in the Room? 

The outcomes of AI are heavily influenced by who builds the technology. Currently, there is a "diversity crisis" in AI, with women making up only about 12% of machine learning engineers. 

The "Invisible" Face: Computer scientist Joy Buolamwini discovered that facial detection software could not even "see" her dark-skinned face until she put on a white mask. Her "Gender Shades" study later proved that these systems were much more accurate for light-skinned men than for dark-skinned women because the people building the systems didn't include enough diverse faces in the training data. 

Automation Bias: People often trust AI more than they trust human experts because they assume a computer is always objective. This is dangerous when biased systems are used to make big decisions, like who gets a loan or who goes to jail. 

Your Online "Bubble" 

For teens, the most common experience with AI bias is the filter bubble. This is "algorithmically curated isolation" where an AI only shows you content that matches what you already believe. This can create an echo chamber, where you never hear different opinions, making the world seem more divided than it actually is. 

The Sandwich Analogy: Building an AI is like writing a recipe for a sandwich. If the chef only asks people who hate pickles what belongs in the "Best Sandwich," the recipe will never include pickles. The recipe isn't "neutral"—it’s just following the narrow instructions and limited ingredients it was given. To make a sandwich that everyone enjoys, you need a diverse group of chefs and a wide variety of ingredients. 